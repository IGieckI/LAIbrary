{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IGieckI/LAIbrary/blob/main/LAIbrary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TpcTxJKph-CT"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3Qr4hJcbh-CU"
      },
      "outputs": [],
      "source": [
        "# Network interfaces\n",
        "\n",
        "class CostFunction:\n",
        "    \"\"\"Define the cost function interface of the network\n",
        "    \"\"\"\n",
        "    def calculate(self, value : list[float], predicted_value : list[float]) -> float:\n",
        "        pass\n",
        "\n",
        "    def calculate_d(self, value : list[float], predicted_value : list[float]) -> float:\n",
        "        pass\n",
        "\n",
        "class ActivationFunction:\n",
        "    \"\"\"Define the activation function interface of the network\n",
        "    \"\"\"\n",
        "    def activation(self, x : list[float]) -> list[float]:\n",
        "        \"\"\"Define the activation function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: output values\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def activation_d(self, x : list[float]) -> list[float]:\n",
        "        \"\"\"Define the derivative of the activation function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: output values\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class Layer:\n",
        "    \"\"\"Define the layer interface of the network\n",
        "    \"\"\"\n",
        "    def __init__(self, cost_function : CostFunction):\n",
        "        self.input , self.output = None, None\n",
        "        self.cost_function = cost_function\n",
        "\n",
        "    def forward(self, input_value : list[float]) -> list[float]:\n",
        "        \"\"\"Define the forward propagation of the layer\n",
        "\n",
        "        Args:\n",
        "            input_value (float): input values\n",
        "\n",
        "        Returns:\n",
        "            float: _description_\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, error : list[float], learning_rate : float) -> list[float]:\n",
        "        \"\"\"Define the backward propagation of the layer\n",
        "\n",
        "        Args:\n",
        "            error (list[float]): error of the previous layer\n",
        "            learning_rate (float): learning rate\n",
        "\n",
        "        Returns:\n",
        "            list[float]: the errors of the next layer\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "de0Wtfy6h-CV"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    def __init__(self, input_indices : list[int], activation_function : ActivationFunction):\n",
        "        self.weights = np.random.rand(len(input_indices)) - 0.5\n",
        "        self.bias = np.random.random() - 0.5\n",
        "        self.input_indices = input_indices\n",
        "\n",
        "        self.last_output = None\n",
        "        self.last_input = None\n",
        "        self.delta = None\n",
        "\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.last_input = inputs\n",
        "\n",
        "        weighted_sum = np.dot(inputs[self.input_indices], self.weights) + self.bias\n",
        "        output = self.activation_function(weighted_sum)\n",
        "\n",
        "        self.last_output = output\n",
        "        return output\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        self.delta = output_error * self.activation_function.activation_d(self.last_output)\n",
        "\n",
        "        # Update the weights and bias\n",
        "        self.weights += learning_rate * np.dot(self.delta, self.last_input[self.input_indices])\n",
        "        self.bias += learning_rate * self.delta\n",
        "\n",
        "    def get_input_indices(self):\n",
        "        return self.input_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "inE-rwhyh-CV"
      },
      "outputs": [],
      "source": [
        "# Connection layers\n",
        "\n",
        "class FullyConnectedLayer(Layer):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "        return np.dot(self.input, self.weights) + self.bias\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error\n",
        "\n",
        "class NotFullyConnectedLayer(Layer):\n",
        "    def __init__(self, cost_function: CostFunction, input_indices: list[list[int]]):\n",
        "        super().__init__(cost_function)\n",
        "        self.input_indices = input_indices\n",
        "        self.neurons = [Neuron(indices) for indices in input_indices]\n",
        "\n",
        "    def forward(self, input_value: list[float]) -> list[float]:\n",
        "        self.input = input_value\n",
        "        outputs = [neuron.forward(input_value) for neuron in self.neurons]\n",
        "        self.output = outputs\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, error: list[float], learning_rate: float) -> list[float]:\n",
        "        prev_layer_error = np.zeros(len(self.input))\n",
        "\n",
        "        for i, neuron in enumerate(self.neurons):\n",
        "            neuron_error = error[i]\n",
        "            prev_layer_error[neuron.input_indices] += neuron.backward(neuron_error, learning_rate, self.input)\n",
        "\n",
        "        return prev_layer_error\n",
        "\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation_function : ActivationFunction):\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "        return self.activation_function.activation(input_data)\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        return self.activation_function.activation_d(self.input) * output_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Md6r7uayh-CV"
      },
      "outputs": [],
      "source": [
        "# Cost functions\n",
        "\n",
        "class MeanError(CostFunction):\n",
        "    @staticmethod\n",
        "    def calculate(value, predicted_value):\n",
        "        return np.mean(value - predicted_value)\n",
        "\n",
        "    def calculate_prime(value, predicted_value):\n",
        "        return 1 / value.size;\n",
        "\n",
        "class MeanSquaredError(CostFunction):\n",
        "    @staticmethod\n",
        "    def calculate(value, predicted_value):\n",
        "        return np.mean(np.power(value - predicted_value, 2));\n",
        "\n",
        "    def calculate_d(value, predicted_value):\n",
        "        return 2*(predicted_value - value) / value.size;\n",
        "\n",
        "# To be implemented\n",
        "def mean_absolute_error(value, predicted_value):\n",
        "    return np.mean(np.abs(value - predicted_value));\n",
        "\n",
        "def log_loss(value, predicted_value, epsilon=1e-15):\n",
        "    # Clip predicted_value to avoid log(0) and log(1) issues\n",
        "    predicted_value = np.clip(predicted_value, epsilon, max(predicted_value))\n",
        "\n",
        "    return -np.mean(value * np.log(predicted_value) + (1 - value) * np.log(1 - predicted_value))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "HoEcDsM0h-CW"
      },
      "outputs": [],
      "source": [
        "# Activation functions\n",
        "\n",
        "# --------BINARY STEP FUNCTIONS---------- #\n",
        "# Binary step function depends on a threshold value that decides whether a neuron should be activated or not.\n",
        "\n",
        "\n",
        "class BinaryStep(ActivationFunction):\n",
        "    @staticmethod\n",
        "    def activation(x : list[float]) -> list[float]:\n",
        "        \"\"\"Binary step function (map input values to 0 or 1)\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: binary step values\n",
        "\n",
        "        notes: not really useful, doesn't provide multi values output, and doesn't provide a gradient (always 0)\n",
        "        \"\"\"\n",
        "        return np.array(0 if i < 0 else 1 for i in x)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_d(x : list[float]) -> list[float]:\n",
        "        \"\"\"Binary step derivative function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: binary step derivative values\n",
        "        \"\"\"\n",
        "        return np.array(0 for i in x)\n",
        "\n",
        "# --------LINEAR FUNCTIONS---------- #\n",
        "#The function doesn't do anything to the weighted sum of the input, it simply spits out the value it was given.\n",
        "\n",
        "class Linear(ActivationFunction):\n",
        "    @staticmethod\n",
        "    def activation(x : list[float]) -> list[float]:\n",
        "        \"\"\"Linear function (map input values to themselves, linear regression)\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: linear values\n",
        "\n",
        "        note: note really useful, the derivative is mostly often not related to the input value, so the gradient descent is not really efficient.\n",
        "            By using this somewhere in the network, no matter the number of layers in the neural network, the last layer will be a linear function of the first layer\n",
        "            so basically the network became a 1 layer network.\n",
        "        \"\"\"\n",
        "        return np.array(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_d(x : list[float]) -> list[float]:\n",
        "        \"\"\"Linear derivative function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: linear derivative values\n",
        "        \"\"\"\n",
        "        return np.array(1 for i in x)\n",
        "\n",
        "# --------NON LINEAR FUNCTIONS---------- #\n",
        "# These allow backpropagation, making possible to create complex mappings between the input and the output of the network.\n",
        "\n",
        "class Sigmoid(ActivationFunction):\n",
        "    @staticmethod\n",
        "    def activation(x : list[float]) -> list[float]:\n",
        "        \"\"\"Sigmoid function (map input values to an S shaped between 0 and 1)\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: sigmoid values\n",
        "\n",
        "        note: Useful since have a smooth (no jumps) shape and map values between 0 and 1, so it can be used as a probability function;\n",
        "            also if something is more to positive will be a lot more towards 1 then something negative.\n",
        "            BUT the derivarive have a significative value only between -3 and 3, then it's almost 0 (GD should be used only on this range),\n",
        "            also suffer of Vanishing gradient problem: when values are really close to 0 or very far from [-3,3] is really difficult to learn\n",
        "            something just from this.\n",
        "        note2: The sigmoid function is not symmetric around zero, which means that the output of the function is always positive, this leads to outputs\n",
        "                with the same sign, either positive or negative, when all neurons have the same sign, it became difficult train the network and less stable.\n",
        "        \"\"\"\n",
        "\n",
        "        return np.array(1 / (1 + np.exp(-i)) for i in x)\n",
        "    @staticmethod\n",
        "    def activation_d(x : list[float]) -> list[float]:\n",
        "        \"\"\"Sigmoid derivative function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: sigmoid derivative values\n",
        "        \"\"\"\n",
        "\n",
        "        return np.array(self.sigmoid(i) * (1 - self.sigmoid(i)) for i in x)\n",
        "\n",
        "class Tanh(ActivationFunction):\n",
        "    @staticmethod\n",
        "    def activation(x : list[float]) -> list[float]:\n",
        "        \"\"\"Tanh function (map input values to an S shaped between -1 and 1, centered at 0)\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: tanh values\n",
        "\n",
        "        note: Similar to sigmoid, but is symmetric around 0, can easly map values as neutral, strongly negative or strongly positive.\n",
        "        note2: Usually used in hidden layers before another layer to help the network learn better.\n",
        "        note3: Also this suffer of Vanishing gradient problem, plus the gradiend is much steeper than sigmoid, this is preferred.\n",
        "        \"\"\"\n",
        "        return np.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_d(x : list[float]) -> list[float]:\n",
        "        \"\"\"Tanh derivative function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: tanh derivative values\n",
        "        \"\"\"\n",
        "        return 1 - np.tanh(x)**2\n",
        "\n",
        "class Relu(ActivationFunction):\n",
        "    @staticmethod\n",
        "    def activation(x : list[float]) -> list[float]:\n",
        "        \"\"\"Relu function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: relu values\n",
        "\n",
        "        note: The interesting thing about this is that this don't activate all neurons at the same time, by this is far more computationally efficient than the others,\n",
        "            also accelerate the convergence due to its linearity property.\n",
        "        note2: The downside is that during backpropagation if the input is negative the gradient is 0, so the neuron is dead and can't learn anything else.\n",
        "        \"\"\"\n",
        "        output = []\n",
        "        for rows in x:\n",
        "            tmp = []\n",
        "            for i in rows:\n",
        "                if i < 0:\n",
        "                    tmp.append(0)\n",
        "                else:\n",
        "                    tmp.append(i)\n",
        "            output.append(tmp)\n",
        "\n",
        "        return np.array(output)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_d(x : list[float]) -> list[float]:\n",
        "        \"\"\"Relu derivative function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: relu derivative values\n",
        "        \"\"\"\n",
        "        output = []\n",
        "        for rows in x:\n",
        "            tmp = []\n",
        "            for i in rows:\n",
        "                if i < 0:\n",
        "                    tmp.append(0)\n",
        "                else:\n",
        "                    tmp.append(1)\n",
        "            output.append(tmp)\n",
        "\n",
        "        return np.array(output)\n",
        "\n",
        "class LeakyRelu(ActivationFunction):\n",
        "    @staticmethod\n",
        "    def activation(x, alpha=0.1):\n",
        "        \"\"\"Leaky/Parametric relu function (similar to relu but don't fully suppress negative values)\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "            alpha (float, optional): negative values suppress parameter. Defaults to 0.1.\n",
        "\n",
        "        Returns:\n",
        "            list[float]: Leaky relu values\n",
        "\n",
        "        note: by having a non 0 value when x < 0, the gradient is not 0, so the neuron is not dead and can learn,\n",
        "            the donwsides are that since the neurons are not dead, they must be computed, also predictions may not be consistent for negative values.\n",
        "        \"\"\"\n",
        "        return np.array(alpha*i if i < 0 else i for i in x)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_d(x, alpha=0.1):\n",
        "        \"\"\"Leaky relu derivative function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "            alpha (float, optional): negative values suppress parameter. Defaults to 0.1.\n",
        "\n",
        "        Returns:\n",
        "            list[float]: Leaky relu derivative values\n",
        "        \"\"\"\n",
        "        return np.array(alpha if i < 0 else 1 for i in x)\n",
        "\n",
        "class Elu(ActivationFunction):\n",
        "    @staticmethod\n",
        "    def activation(x, alpha=0.1):\n",
        "        \"\"\"Elu function (similar to leaky relu but with a smooth curve)\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "            alpha (float, optional): negative values suppress parameter. Defaults to 0.1.\n",
        "\n",
        "        Returns:\n",
        "            list[float]: Elu values\n",
        "\n",
        "        note: strong alternative to parametric relu, but the computation is (a lot) more expensive.\n",
        "        note: This suffer of the exploding gradient problem: since there is an exponential, the gradient can explode and make the network unstable.\n",
        "        \"\"\"\n",
        "        return np.array(alpha*(np.exp(i) - 1) if i < 0 else i for i in x)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_d(x, alpha=0.1):\n",
        "        \"\"\"Elu derivative function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "            alpha (float, optional): negative values suppress parameter. Defaults to 0.1.\n",
        "\n",
        "        Returns:\n",
        "            list[float]: Elu derivative values\n",
        "        \"\"\"\n",
        "        return np.array(alpha + i if i < 0 else 1 for i in x)\n",
        "\n",
        "class Softmax(ActivationFunction):\n",
        "    @staticmethod\n",
        "    def activation(value : list[float]) -> list[float]:\n",
        "        \"\"\"Softmax function (map values to a probability distribution between 0 and 1, is a generalization of the sigmoid function)\n",
        "\n",
        "        Args:\n",
        "            value (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: softmax values\n",
        "\n",
        "        note: A lot common on multi-class classification problems, since the output is a probability distribution, the sum of all values is 1.\n",
        "        \"\"\"\n",
        "        return np.array(np.exp(i) / np.sum(np.exp(value), axis=0) for i in value)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_d(self, value : list[float]) -> list[float]:\n",
        "        \"\"\"Softmax derivative function\n",
        "\n",
        "        Args:\n",
        "            value (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: softmax derivative values\n",
        "\n",
        "        # !!! REVIEW THIS, NOT SURE IF CORRECT, NEED A MATEMATICIAN :( !!!\n",
        "        \"\"\"\n",
        "        return np.array(np.exp(i) / np.sum(np.exp(value), axis=0) for i in value)\n",
        "\n",
        "class Swish(ActivationFunction):\n",
        "    @staticmethod\n",
        "    def activation(x : list[float]) -> list[float]:\n",
        "        \"\"\"Swish function (similar to sigmoid but with a non 0 gradient for negative values)\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: swish values\n",
        "\n",
        "        note: This often match or outperform relu, but is more computationally expensive:\n",
        "            is a smooth function (expecially around 0), the 0 values in relu and other functions are set to 0, but they might be useful,\n",
        "\n",
        "        \"\"\"\n",
        "        return np.array(i / (1 + np.exp(-i)) for i in x)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_d(x : list[float]) -> list[float]:\n",
        "        \"\"\"Swish derivative function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: swish derivative values\n",
        "        \"\"\"\n",
        "        return np.array(Swish.activation(i) + Sigmoid.activation(i) * (1 - Swish.activation(i)) for i in x)\n",
        "\n",
        "class Gelu(ActivationFunction):\n",
        "    @staticmethod\n",
        "    def activation(self, x : list[float]) -> list[float]:\n",
        "        \"\"\"Gelu function (similar to swish but with a different shape)\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: gelu values\n",
        "\n",
        "        note: Has found to outperform other activation functions on computer vision, natural language processing and speech recognition tasks,\n",
        "            the downside is that is more computationally expensive.\n",
        "        \"\"\"\n",
        "        return np.array(0.5 * i * (1 + np.tanh(np.sqrt(2 / np.pi) * (i + 0.044715 * np.power(i, 3)))) for i in x)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation_d(self, x : list[float]) -> list[float]:\n",
        "        \"\"\"Gelu derivative function\n",
        "\n",
        "        Args:\n",
        "            x (list[float]): input values\n",
        "\n",
        "        Returns:\n",
        "            list[float]: gelu derivative values\n",
        "        \"\"\"\n",
        "        return np.array(0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (i + 0.044715 * np.power(i, 3)))) + 0.5 * i * (1 - np.power(np.tanh(np.sqrt(2 / np.pi) * (i + 0.044715 * np.power(i, 3))), 2)) * (np.sqrt(2 / np.pi) * (1 + 0.134145 * np.power(i, 2))) for i in x)\n",
        "\n",
        "# To be implemented\n",
        "\n",
        "def linear_softmax(value : list[float]) -> list[float]:\n",
        "    \"\"\"Linear softmax function (map values to a linear probability distribution between 0 and 1)\n",
        "\n",
        "    Args:\n",
        "        value (list[float]): input values\n",
        "\n",
        "    Returns:\n",
        "        list[float]: linear softmax values\n",
        "    \"\"\"\n",
        "    return value / np.sum(value, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "07mqp499h-CW"
      },
      "outputs": [],
      "source": [
        "# The network\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, cost_function : CostFunction):\n",
        "        self.layers = []\n",
        "        self.cost_function = cost_function\n",
        "\n",
        "    def add_layer(self, layer : Layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, input_data):\n",
        "        result = []\n",
        "\n",
        "        # run network over all samples\n",
        "        for i in range(len(input_data)):\n",
        "            # forward propagation\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward(output)\n",
        "            result.append(output)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def train(self, input, expected_output, epochs, learning_rate):\n",
        "        samples = len(input)\n",
        "\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                output = input[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward(output)\n",
        "                # compute loss (for display purpose only)\n",
        "                err += MeanSquaredError.calculate(output, expected_output[j])\n",
        "\n",
        "                # backward propagation\n",
        "                error = MeanSquaredError.calculate_d(expected_output[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error = layer.backward(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "0O87IC9Kh-CX",
        "outputId": "6d994612-2233-4770-a46a-f3de1e3b0093",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/100   error=666657680912.761475\n",
            "epoch 2/100   error=666657680819.113037\n",
            "epoch 3/100   error=666657680819.113037\n",
            "epoch 4/100   error=666657680819.113037\n",
            "epoch 5/100   error=666657680819.113037\n",
            "epoch 6/100   error=666657680819.113037\n",
            "epoch 7/100   error=666657680819.113037\n",
            "epoch 8/100   error=666657680819.113037\n",
            "epoch 9/100   error=666657680819.113037\n",
            "epoch 10/100   error=666657680819.113037\n",
            "epoch 11/100   error=666657680819.113037\n",
            "epoch 12/100   error=666657680819.113037\n",
            "epoch 13/100   error=666657680819.113037\n",
            "epoch 14/100   error=666657680819.113037\n",
            "epoch 15/100   error=666657680819.113037\n",
            "epoch 16/100   error=666657680819.113037\n",
            "epoch 17/100   error=666657680819.113037\n",
            "epoch 18/100   error=666657680819.113037\n",
            "epoch 19/100   error=666657680819.113037\n",
            "epoch 20/100   error=666657680819.113037\n",
            "epoch 21/100   error=666657680819.113037\n",
            "epoch 22/100   error=666657680819.113037\n",
            "epoch 23/100   error=666657680819.113037\n",
            "epoch 24/100   error=666657680819.113037\n",
            "epoch 25/100   error=666657680819.113037\n",
            "epoch 26/100   error=666657680819.113037\n",
            "epoch 27/100   error=666657680819.113037\n",
            "epoch 28/100   error=666657680819.113037\n",
            "epoch 29/100   error=666657680819.113037\n",
            "epoch 30/100   error=666657680819.113037\n",
            "epoch 31/100   error=666657680819.113037\n",
            "epoch 32/100   error=666657680819.113037\n",
            "epoch 33/100   error=666657680819.113037\n",
            "epoch 34/100   error=666657680819.113037\n",
            "epoch 35/100   error=666657680819.113037\n",
            "epoch 36/100   error=666657680819.113037\n",
            "epoch 37/100   error=666657680819.113037\n",
            "epoch 38/100   error=666657680819.113037\n",
            "epoch 39/100   error=666657680819.113037\n",
            "epoch 40/100   error=666657680819.113037\n",
            "epoch 41/100   error=666657680819.113037\n",
            "epoch 42/100   error=666657680819.113037\n",
            "epoch 43/100   error=666657680819.113037\n",
            "epoch 44/100   error=666657680819.113037\n",
            "epoch 45/100   error=666657680819.113037\n",
            "epoch 46/100   error=666657680819.113037\n",
            "epoch 47/100   error=666657680819.113037\n",
            "epoch 48/100   error=666657680819.113037\n",
            "epoch 49/100   error=666657680819.113037\n",
            "epoch 50/100   error=666657680819.113037\n",
            "epoch 51/100   error=666657680819.113037\n",
            "epoch 52/100   error=666657680819.113037\n",
            "epoch 53/100   error=666657680819.113037\n",
            "epoch 54/100   error=666657680819.113037\n",
            "epoch 55/100   error=666657680819.113037\n",
            "epoch 56/100   error=666657680819.113037\n",
            "epoch 57/100   error=666657680819.113037\n",
            "epoch 58/100   error=666657680819.113037\n",
            "epoch 59/100   error=666657680819.113037\n",
            "epoch 60/100   error=666657680819.113037\n",
            "epoch 61/100   error=666657680819.113037\n",
            "epoch 62/100   error=666657680819.113037\n",
            "epoch 63/100   error=666657680819.113037\n",
            "epoch 64/100   error=666657680819.113037\n",
            "epoch 65/100   error=666657680819.113037\n",
            "epoch 66/100   error=666657680819.113037\n",
            "epoch 67/100   error=666657680819.113037\n",
            "epoch 68/100   error=666657680819.113037\n",
            "epoch 69/100   error=666657680819.113037\n",
            "epoch 70/100   error=666657680819.113037\n",
            "epoch 71/100   error=666657680819.113037\n",
            "epoch 72/100   error=666657680819.113037\n",
            "epoch 73/100   error=666657680819.113037\n",
            "epoch 74/100   error=666657680819.113037\n",
            "epoch 75/100   error=666657680819.113037\n",
            "epoch 76/100   error=666657680819.113037\n",
            "epoch 77/100   error=666657680819.113037\n",
            "epoch 78/100   error=666657680819.113037\n",
            "epoch 79/100   error=666657680819.113037\n",
            "epoch 80/100   error=666657680819.113037\n",
            "epoch 81/100   error=666657680819.113037\n",
            "epoch 82/100   error=666657680819.113037\n",
            "epoch 83/100   error=666657680819.113037\n",
            "epoch 84/100   error=666657680819.113037\n",
            "epoch 85/100   error=666657680819.113037\n",
            "epoch 86/100   error=666657680819.113037\n",
            "epoch 87/100   error=666657680819.113037\n",
            "epoch 88/100   error=666657680819.113037\n",
            "epoch 89/100   error=666657680819.113037\n",
            "epoch 90/100   error=666657680819.113037\n",
            "epoch 91/100   error=666657680819.113037\n",
            "epoch 92/100   error=666657680819.113037\n",
            "epoch 93/100   error=666657680819.113037\n",
            "epoch 94/100   error=666657680819.113037\n",
            "epoch 95/100   error=666657680819.113037\n",
            "epoch 96/100   error=666657680819.113037\n",
            "epoch 97/100   error=666657680819.113037\n",
            "epoch 98/100   error=666657680819.113037\n",
            "epoch 99/100   error=666657680819.113037\n",
            "epoch 100/100   error=666657680819.113037\n",
            "[array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[-1.]]), array([[1.]]), array([[1.]]), array([[1.]])]\n"
          ]
        }
      ],
      "source": [
        "# Training test 1\n",
        "lstx, lsty = [], []\n",
        "for x in range(1000):\n",
        "\tx1, x2 = random.randint(-1000000,1000000), random.randint(-1000000,1000000)\n",
        "\tlstx.append([[x1, x2]])\n",
        "\tlsty.append([[x1+x2]])\n",
        "\n",
        "x_train = np.array(lstx)\n",
        "y_train = np.array(lsty)\n",
        "\n",
        "# network\n",
        "net = Network(MeanSquaredError())\n",
        "net.add_layer(FullyConnectedLayer(2, 50))\n",
        "net.add_layer(ActivationLayer(Tanh))\n",
        "net.add_layer(FullyConnectedLayer(50, 1))\n",
        "net.add_layer(ActivationLayer(Tanh))\n",
        "# train\n",
        "net.train(x_train, y_train, epochs=100, learning_rate=0.1)\n",
        "\n",
        "# test\n",
        "out = net.predict(x_train)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "iXgptsCOh-CX"
      },
      "outputs": [],
      "source": [
        "# Function plotter\n",
        "\n",
        "def plot_func(x,y, title):\n",
        "    # helper function to plot activation functions\n",
        "    plt.plot(x, y)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('activation(x)')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}