{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network interfaces\n",
    "    \n",
    "class CostFunction:\n",
    "    \"\"\"Define the cost function interface of the network\n",
    "    \"\"\"\n",
    "    def calculate(self, value : list[float], predicted_value : list[float]) -> float:\n",
    "        pass\n",
    "\n",
    "    def calculate_d(self, value : list[float], predicted_value : list[float]) -> float:\n",
    "        pass\n",
    "    \n",
    "class ActivationFunction:\n",
    "    \"\"\"Define the activation function interface of the network\n",
    "    \"\"\"\n",
    "    def activation(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Define the activation function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: output values\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def activation_d(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Define the derivative of the activation function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: output values\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class Layer:\n",
    "    \"\"\"Define the layer interface of the network\n",
    "    \"\"\"\n",
    "    def __init__(self, cost_function : CostFunction):\n",
    "        self.input , self.output = None, None\n",
    "        self.cost_function = cost_function\n",
    "\n",
    "    def forward(self, input_value : list[float]) -> list[float]:\n",
    "        \"\"\"Define the forward propagation of the layer\n",
    "\n",
    "        Args:\n",
    "            input_value (float): input values\n",
    "\n",
    "        Returns:\n",
    "            float: _description_\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backward(self, error : list[float], learning_rate : float) -> list[float]:\n",
    "        \"\"\"Define the backward propagation of the layer\n",
    "\n",
    "        Args:\n",
    "            error (list[float]): error of the previous layer\n",
    "            learning_rate (float): learning rate\n",
    "\n",
    "        Returns:\n",
    "            list[float]: the errors of the next layer\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_indices : list[int], activation_function : ActivationFunction):\n",
    "        self.weights = np.random.rand(len(input_indices)) - 0.5\n",
    "        self.bias = np.random.random() - 0.5\n",
    "        self.input_indices = input_indices\n",
    "        \n",
    "        self.last_output = None\n",
    "        self.last_input = None\n",
    "        self.delta = None\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.last_input = inputs\n",
    "        \n",
    "        weighted_sum = np.dot(inputs[self.input_indices], self.weights) + self.bias\n",
    "        output = self.activation_function(weighted_sum)\n",
    "        \n",
    "        self.last_output = output\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):   \n",
    "        self.delta = output_error * self.activation_function.activation_d(self.last_output)\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        self.weights += learning_rate * np.dot(self.delta, self.last_input[self.input_indices])\n",
    "        self.bias += learning_rate * self.delta\n",
    "\n",
    "    def get_input_indices(self):\n",
    "        return self.input_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection layers\n",
    "\n",
    "class FullyConnectedLayer(Layer):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(self.input, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        \n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "\n",
    "class NotFullyConnectedLayer(Layer):\n",
    "    def __init__(self, cost_function: CostFunction, input_indices: list[list[int]]):\n",
    "        super().__init__(cost_function)\n",
    "        self.input_indices = input_indices\n",
    "        self.neurons = [Neuron(indices) for indices in input_indices]\n",
    "\n",
    "    def forward(self, input_value: list[float]) -> list[float]:\n",
    "        self.input = input_value\n",
    "        outputs = [neuron.forward(input_value) for neuron in self.neurons]\n",
    "        self.output = outputs\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, error: list[float], learning_rate: float) -> list[float]:\n",
    "        prev_layer_error = np.zeros(len(self.input))\n",
    "\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron_error = error[i]\n",
    "            prev_layer_error[neuron.input_indices] += neuron.backward(neuron_error, learning_rate, self.input)\n",
    "\n",
    "        return prev_layer_error\n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation_function : ActivationFunction):\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        return self.activation_function.activation(input_data)\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        return self.activation_function.activation_d(self.input) * output_error"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
