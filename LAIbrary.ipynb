{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network interfaces\n",
    "    \n",
    "class CostFunction:\n",
    "    \"\"\"Define the cost function interface of the network\n",
    "    \"\"\"\n",
    "    def calculate(self, value : list[float], predicted_value : list[float]) -> float:\n",
    "        pass\n",
    "\n",
    "    def calculate_d(self, value : list[float], predicted_value : list[float]) -> float:\n",
    "        pass\n",
    "    \n",
    "class ActivationFunction:\n",
    "    \"\"\"Define the activation function interface of the network\n",
    "    \"\"\"\n",
    "    def activation(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Define the activation function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: output values\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def activation_d(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Define the derivative of the activation function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: output values\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class Layer:\n",
    "    \"\"\"Define the layer interface of the network\n",
    "    \"\"\"\n",
    "    def __init__(self, cost_function : CostFunction):\n",
    "        self.input , self.output = None, None\n",
    "        self.cost_function = cost_function\n",
    "\n",
    "    def forward(self, input_value : list[float]) -> list[float]:\n",
    "        \"\"\"Define the forward propagation of the layer\n",
    "\n",
    "        Args:\n",
    "            input_value (float): input values\n",
    "\n",
    "        Returns:\n",
    "            float: _description_\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backward(self, error : list[float], learning_rate : float) -> list[float]:\n",
    "        \"\"\"Define the backward propagation of the layer\n",
    "\n",
    "        Args:\n",
    "            error (list[float]): error of the previous layer\n",
    "            learning_rate (float): learning rate\n",
    "\n",
    "        Returns:\n",
    "            list[float]: the errors of the next layer\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_indices : list[int], activation_function : ActivationFunction):\n",
    "        self.weights = np.random.rand(len(input_indices)) - 0.5\n",
    "        self.bias = np.random.random() - 0.5\n",
    "        self.input_indices = input_indices\n",
    "        \n",
    "        self.last_output = None\n",
    "        self.last_input = None\n",
    "        self.delta = None\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.last_input = inputs\n",
    "        \n",
    "        weighted_sum = np.dot(inputs[self.input_indices], self.weights) + self.bias\n",
    "        output = self.activation_function(weighted_sum)\n",
    "        \n",
    "        self.last_output = output\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):   \n",
    "        self.delta = output_error * self.activation_function.activation_d(self.last_output)\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        self.weights += learning_rate * np.dot(self.delta, self.last_input[self.input_indices])\n",
    "        self.bias += learning_rate * self.delta\n",
    "\n",
    "    def get_input_indices(self):\n",
    "        return self.input_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection layers\n",
    "\n",
    "class FullyConnectedLayer(Layer):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(self.input, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        \n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "\n",
    "class NotFullyConnectedLayer(Layer):\n",
    "    def __init__(self, cost_function: CostFunction, input_indices: list[list[int]]):\n",
    "        super().__init__(cost_function)\n",
    "        self.input_indices = input_indices\n",
    "        self.neurons = [Neuron(indices) for indices in input_indices]\n",
    "\n",
    "    def forward(self, input_value: list[float]) -> list[float]:\n",
    "        self.input = input_value\n",
    "        outputs = [neuron.forward(input_value) for neuron in self.neurons]\n",
    "        self.output = outputs\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, error: list[float], learning_rate: float) -> list[float]:\n",
    "        prev_layer_error = np.zeros(len(self.input))\n",
    "\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron_error = error[i]\n",
    "            prev_layer_error[neuron.input_indices] += neuron.backward(neuron_error, learning_rate, self.input)\n",
    "\n",
    "        return prev_layer_error\n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation_function : ActivationFunction):\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        return self.activation_function.activation(input_data)\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        return self.activation_function.activation_d(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost functions\n",
    "\n",
    "class MeanError(CostFunction):\n",
    "    @staticmethod\n",
    "    def calculate(value, predicted_value):\n",
    "        return np.mean(value - predicted_value)\n",
    "\n",
    "    def calculate_prime(value, predicted_value):\n",
    "        return 1 / value.size;\n",
    "\n",
    "class MeanSquaredError(CostFunction):\n",
    "    @staticmethod\n",
    "    def calculate(value, predicted_value):\n",
    "        return np.mean(np.power(value - predicted_value, 2));\n",
    "\n",
    "    def calculate_d(value, predicted_value):\n",
    "        return 2*(predicted_value - value) / value.size;\n",
    "\n",
    "# To be implemented\n",
    "def mean_absolute_error(value, predicted_value):\n",
    "    return np.mean(np.abs(value - predicted_value));\n",
    "\n",
    "def log_loss(value, predicted_value, epsilon=1e-15):\n",
    "    # Clip predicted_value to avoid log(0) and log(1) issues\n",
    "    predicted_value = np.clip(predicted_value, epsilon, max(predicted_value))\n",
    "    \n",
    "    return -np.mean(value * np.log(predicted_value) + (1 - value) * np.log(1 - predicted_value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "# --------BINARY STEP FUNCTIONS---------- #\n",
    "# Binary step function depends on a threshold value that decides whether a neuron should be activated or not.\n",
    "\n",
    "\n",
    "class BinaryStep(ActivationFunction):\n",
    "    def activation(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Binary step function (map input values to 0 or 1) \n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: binary step values\n",
    "        \n",
    "        notes: not really useful, doesn't provide multi values output, and doesn't provide a gradient (always 0)\n",
    "        \"\"\"\n",
    "        return np.array(0 if i < 0 else 1 for i in x)\n",
    "\n",
    "    def activation_d(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Binary step derivative function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: binary step derivative values\n",
    "        \"\"\"\n",
    "        return np.array(0 for i in x)\n",
    "\n",
    "# --------LINEAR FUNCTIONS---------- #\n",
    "#The function doesn't do anything to the weighted sum of the input, it simply spits out the value it was given.\n",
    "\n",
    "class Linear(ActivationFunction):\n",
    "    def activation(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Linear function (map input values to themselves, linear regression)\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: linear values\n",
    "            \n",
    "        note: note really useful, the derivative is mostly often not related to the input value, so the gradient descent is not really efficient.\n",
    "            By using this somewhere in the network, no matter the number of layers in the neural network, the last layer will be a linear function of the first layer\n",
    "            so basically the network became a 1 layer network.\n",
    "        \"\"\"\n",
    "        return np.array(x)\n",
    "\n",
    "    def activation_d(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Linear derivative function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: linear derivative values\n",
    "        \"\"\"\n",
    "        return np.array(1 for i in x)\n",
    "\n",
    "# --------NON LINEAR FUNCTIONS---------- #\n",
    "# These allow backpropagation, making possible to create complex mappings between the input and the output of the network.\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def activation(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Sigmoid function (map input values to an S shaped between 0 and 1)\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: sigmoid values\n",
    "            \n",
    "        note: Useful since have a smooth (no jumps) shape and map values between 0 and 1, so it can be used as a probability function;\n",
    "            also if something is more to positive will be a lot more towards 1 then something negative.\n",
    "            BUT the derivarive have a significative value only between -3 and 3, then it's almost 0 (GD should be used only on this range),\n",
    "            also suffer of Vanishing gradient problem: when values are really close to 0 or very far from [-3,3] is really difficult to learn\n",
    "            something just from this.\n",
    "        note2: The sigmoid function is not symmetric around zero, which means that the output of the function is always positive, this leads to outputs \n",
    "                with the same sign, either positive or negative, when all neurons have the same sign, it became difficult train the network and less stable.\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.array(1 / (1 + np.exp(-i)) for i in x)\n",
    "\n",
    "    def activation_d(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Sigmoid derivative function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: sigmoid derivative values\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.array(self.sigmoid(i) * (1 - self.sigmoid(i)) for i in x)\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def activation(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Tanh function (map input values to an S shaped between -1 and 1, centered at 0)\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: tanh values\n",
    "            \n",
    "        note: Similar to sigmoid, but is symmetric around 0, can easly map values as neutral, strongly negative or strongly positive.\n",
    "        note2: Usually used in hidden layers before another layer to help the network learn better.\n",
    "        note3: Also this suffer of Vanishing gradient problem, plus the gradiend is much steeper than sigmoid, this is preferred.\n",
    "        \"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def activation_d(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Tanh derivative function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: tanh derivative values\n",
    "        \"\"\"\n",
    "        return 1 - np.tanh(x)**2\n",
    "\n",
    "class Relu(ActivationFunction):\n",
    "    def activation(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Relu function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: relu values\n",
    "            \n",
    "        note: The interesting thing about this is that this don't activate all neurons at the same time, by this is far more computationally efficient than the others,\n",
    "            also accelerate the convergence due to its linearity property.\n",
    "        note2: The downside is that during backpropagation if the input is negative the gradient is 0, so the neuron is dead and can't learn anything else.\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for rows in x:            \n",
    "            tmp = []\n",
    "            for i in rows:\n",
    "                if i < 0:\n",
    "                    tmp.append(0)\n",
    "                else:\n",
    "                    tmp.append(i)\n",
    "            output.append(tmp)\n",
    "                    \n",
    "        return np.array(output)\n",
    "\n",
    "    def activation_d(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Relu derivative function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: relu derivative values\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for rows in x:            \n",
    "            tmp = []\n",
    "            for i in rows:\n",
    "                if i < 0:\n",
    "                    tmp.append(0)\n",
    "                else:\n",
    "                    tmp.append(1)\n",
    "            output.append(tmp)\n",
    "                    \n",
    "        return np.array(output)\n",
    "\n",
    "class LeakyRelu(ActivationFunction):\n",
    "    def activation(self, x, alpha=0.1):\n",
    "        \"\"\"Leaky/Parametric relu function (similar to relu but don't fully suppress negative values)\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "            alpha (float, optional): negative values suppress parameter. Defaults to 0.1.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: Leaky relu values\n",
    "            \n",
    "        note: by having a non 0 value when x < 0, the gradient is not 0, so the neuron is not dead and can learn,\n",
    "            the donwsides are that since the neurons are not dead, they must be computed, also predictions may not be consistent for negative values.\n",
    "        \"\"\"\n",
    "        return np.array(alpha*i if i < 0 else i for i in x)\n",
    "\n",
    "    def activation_d(self, x, alpha=0.1):\n",
    "        \"\"\"Leaky relu derivative function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "            alpha (float, optional): negative values suppress parameter. Defaults to 0.1.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: Leaky relu derivative values\n",
    "        \"\"\"\n",
    "        return np.array(alpha if i < 0 else 1 for i in x)\n",
    "\n",
    "class Elu(ActivationFunction):\n",
    "    def activation(self, x, alpha=0.1):\n",
    "        \"\"\"Elu function (similar to leaky relu but with a smooth curve)\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "            alpha (float, optional): negative values suppress parameter. Defaults to 0.1.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: Elu values\n",
    "            \n",
    "        note: strong alternative to parametric relu, but the computation is (a lot) more expensive.\n",
    "        note: This suffer of the exploding gradient problem: since there is an exponential, the gradient can explode and make the network unstable.\n",
    "        \"\"\"\n",
    "        return np.array(alpha*(np.exp(i) - 1) if i < 0 else i for i in x)\n",
    "\n",
    "    def activation_d(self, x, alpha=0.1):\n",
    "        \"\"\"Elu derivative function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "            alpha (float, optional): negative values suppress parameter. Defaults to 0.1.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: Elu derivative values\n",
    "        \"\"\"\n",
    "        return np.array(alpha + i if i < 0 else 1 for i in x)\n",
    "\n",
    "class Softmax(ActivationFunction):\n",
    "    def activation(self, value : list[float]) -> list[float]:\n",
    "        \"\"\"Softmax function (map values to a probability distribution between 0 and 1, is a generalization of the sigmoid function)\n",
    "\n",
    "        Args:\n",
    "            value (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: softmax values\n",
    "            \n",
    "        note: A lot common on multi-class classification problems, since the output is a probability distribution, the sum of all values is 1.\n",
    "        \"\"\"\n",
    "        return np.array(np.exp(i) / np.sum(np.exp(value), axis=0) for i in value)\n",
    "\n",
    "    def activation_d(self, value : list[float]) -> list[float]:\n",
    "        \"\"\"Softmax derivative function\n",
    "\n",
    "        Args:\n",
    "            value (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: softmax derivative values\n",
    "        \n",
    "        # !!! REVIEW THIS, NOT SURE IF CORRECT, NEED A MATEMATICIAN :( !!!\n",
    "        \"\"\"\n",
    "        return np.array(np.exp(i) / np.sum(np.exp(value), axis=0) for i in value)\n",
    "\n",
    "class Swish(ActivationFunction):\n",
    "    def activation(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Swish function (similar to sigmoid but with a non 0 gradient for negative values)\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: swish values\n",
    "            \n",
    "        note: This often match or outperform relu, but is more computationally expensive: \n",
    "            is a smooth function (expecially around 0), the 0 values in relu and other functions are set to 0, but they might be useful,\n",
    "            \n",
    "        \"\"\"\n",
    "        return np.array(i / (1 + np.exp(-i)) for i in x)\n",
    "\n",
    "    def activation_d(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Swish derivative function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: swish derivative values\n",
    "        \"\"\"\n",
    "        return np.array(self.swish(i) + self.sigmoid(i) * (1 - self.swish(i)) for i in x)\n",
    "\n",
    "class Gelu(ActivationFunction):\n",
    "    def activation(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Gelu function (similar to swish but with a different shape)\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: gelu values\n",
    "            \n",
    "        note: Has found to outperform other activation functions on computer vision, natural language processing and speech recognition tasks,\n",
    "            the downside is that is more computationally expensive.\n",
    "        \"\"\"\n",
    "        return np.array(0.5 * i * (1 + np.tanh(np.sqrt(2 / np.pi) * (i + 0.044715 * np.power(i, 3)))) for i in x)\n",
    "\n",
    "    def activation_d(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Gelu derivative function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: gelu derivative values\n",
    "        \"\"\"\n",
    "        return np.array(0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (i + 0.044715 * np.power(i, 3)))) + 0.5 * i * (1 - np.power(np.tanh(np.sqrt(2 / np.pi) * (i + 0.044715 * np.power(i, 3))), 2)) * (np.sqrt(2 / np.pi) * (1 + 0.134145 * np.power(i, 2))) for i in x)\n",
    "    \n",
    "# To be implemented\n",
    "\n",
    "def linear_softmax(value : list[float]) -> list[float]:\n",
    "    \"\"\"Linear softmax function (map values to a linear probability distribution between 0 and 1)\n",
    "\n",
    "    Args:\n",
    "        value (list[float]): input values\n",
    "\n",
    "    Returns:\n",
    "        list[float]: linear softmax values\n",
    "    \"\"\"\n",
    "    return value / np.sum(value, axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
