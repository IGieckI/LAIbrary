{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network interfaces\n",
    "    \n",
    "class CostFunction:\n",
    "    \"\"\"Define the cost function interface of the network\n",
    "    \"\"\"\n",
    "    def calculate(self, value : list[float], predicted_value : list[float]) -> float:\n",
    "        pass\n",
    "\n",
    "    def calculate_d(self, value : list[float], predicted_value : list[float]) -> float:\n",
    "        pass\n",
    "    \n",
    "class ActivationFunction:\n",
    "    \"\"\"Define the activation function interface of the network\n",
    "    \"\"\"\n",
    "    def activation(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Define the activation function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: output values\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def activation_d(self, x : list[float]) -> list[float]:\n",
    "        \"\"\"Define the derivative of the activation function\n",
    "\n",
    "        Args:\n",
    "            x (list[float]): input values\n",
    "\n",
    "        Returns:\n",
    "            list[float]: output values\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class Layer:\n",
    "    \"\"\"Define the layer interface of the network\n",
    "    \"\"\"\n",
    "    def __init__(self, cost_function : CostFunction):\n",
    "        self.input , self.output = None, None\n",
    "        self.cost_function = cost_function\n",
    "\n",
    "    def forward(self, input_value : list[float]) -> list[float]:\n",
    "        \"\"\"Define the forward propagation of the layer\n",
    "\n",
    "        Args:\n",
    "            input_value (float): input values\n",
    "\n",
    "        Returns:\n",
    "            float: _description_\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backward(self, error : list[float], learning_rate : float) -> list[float]:\n",
    "        \"\"\"Define the backward propagation of the layer\n",
    "\n",
    "        Args:\n",
    "            error (list[float]): error of the previous layer\n",
    "            learning_rate (float): learning rate\n",
    "\n",
    "        Returns:\n",
    "            list[float]: the errors of the next layer\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_indices : list[int], activation_function : ActivationFunction):\n",
    "        self.weights = np.random.rand(len(input_indices)) - 0.5\n",
    "        self.bias = np.random.random() - 0.5\n",
    "        self.input_indices = input_indices\n",
    "        \n",
    "        self.last_output = None\n",
    "        self.last_input = None\n",
    "        self.delta = None\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.last_input = inputs\n",
    "        \n",
    "        weighted_sum = np.dot(inputs[self.input_indices], self.weights) + self.bias\n",
    "        output = self.activation_function(weighted_sum)\n",
    "        \n",
    "        self.last_output = output\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):   \n",
    "        self.delta = output_error * self.activation_function.activation_d(self.last_output)\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        self.weights += learning_rate * np.dot(self.delta, self.last_input[self.input_indices])\n",
    "        self.bias += learning_rate * self.delta\n",
    "\n",
    "    def get_input_indices(self):\n",
    "        return self.input_indices"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
